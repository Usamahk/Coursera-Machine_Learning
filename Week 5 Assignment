Last login: Sun Jan  3 01:05:34 on console
Usamahs-MacBook-Pro:~ Usamahk$ /usr/local/octave/3.8.0/bin/octave-3.8.0 ; exit;
GNU Octave, version 3.8.0
Copyright (C) 2013 John W. Eaton and others.
This is free software; see the source code for copying conditions.
There is ABSOLUTELY NO WARRANTY; not even for MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE.  For details, type 'warranty'.

Octave was configured for "x86_64-apple-darwin13.0.0".

Additional information about Octave is available at http://www.octave.org.

Please contribute if you find this software useful.
For more information, visit http://www.octave.org/get-involved.html

Read http://www.octave.org/bugs.html to learn how to submit bug reports.
For information about changes from previous versions, type 'news'.

octave:1> PS1 ('>> ')
>> 2*1.001^4 + 2
ans =  4.0080
>> 2*0.99^4 + 2
ans =  3.9212
>> a = 2*1.001^4 + 2
a =  4.0080
>> b = 2*0.99^4 + 2
b =  3.9212
>> whos
Variables in the current scope:

   Attr Name        Size                     Bytes  Class
   ==== ====        ====                     =====  ===== 
        a           1x1                          8  double
        ans         1x1                          8  double
        b           1x1                          8  double

Total is 3 elements using 24 bytes

>> ls
Admin		Documents	Library		Pictures	anaconda
Applications	Downloads	Movies		Public
Desktop		For Haya	Music		TV Shows
>> (a - b)/0.02
ans =  4.3410
>> a- b
ans =  0.086820
>> c= a-b
c =  0.086820
>> c/0.02
ans =  4.3410
>> b = 2*(0.99)^4 + 2
b =  3.9212
>> b = 2*((0.99)^4) + 2
b =  3.9212
>> b = 2*((1.01)^4) + 2
b =  4.0812
>> a = 2*((1.01)^4) + 2
a =  4.0812
>> b = 2*((0.99)^4) + 2
b =  3.9212
>> a - b
ans =  0.16002
>> c = a - b
c =  0.16002
>> c/0.02
ans =  8.0008
>> pwd
ans = /Users/Usamahk
>> cd '~/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4'
>> ls
checkNNGradients.m		lib
computeNumericalGradient.m	nnCostFunction.m
debugInitializeWeights.m	predict.m
displayData.m			randInitializeWeights.m
ex4.m				sigmoid.m
ex4data1.mat			sigmoidGradient.m
ex4weights.mat			submit.m
fmincg.m
>> ex4()
error: invalid use of script /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m in index expression
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
error: nnCostFunction: operator *: nonconformant arguments (op1 is 5000x400, op2 is 10285x1)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 65, column 3
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> ex4







Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.000000 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.000000 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.000000 0.000000 0.000000 0.000000 0.000000 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
warning: division by zero
If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: NaN

Program paused. Press enter to continue.

Checking Backpropagation (w/ Regularization) ... 
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
   0   0
warning: division by zero
If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: NaN


Cost at (fixed) debugging parameters (w/ lambda = 10): 0.000000 
(this value should be about 0.576051)

Program paused. Press enter to continue.

Training Neural Network... 
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero

Program paused. Press enter to continue.

Visualizing Neural Network... 
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ls
checkNNGradients.m		lib
computeNumericalGradient.m	nnCostFunction.m
debugInitializeWeights.m	predict.m
displayData.m			randInitializeWeights.m
ex4.m				sigmoid.m
ex4data1.mat			sigmoidGradient.m
ex4weights.mat			submit.m
fmincg.m
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
-- less -- (f)orward, (b)ack, (q)uit
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

~
~
-- less (100%) (f)orward, (b)ack, (q)uit
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

~
~
~
~
~
~
-- less (100%) (f)orward, (b)ack, (q)uit
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

~
~
~
~
~
~
~
~
~
~
~
~
~
~
-- less (100%) (f)orward, (b)ack, (q)uit
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
-- less (100%) (f)orward, (b)ack, (q)uit
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
~
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> J = ( (1 / m) * sum(-y'*log(a3)) - (1-y)'*log( 1 - sigmoid(a3))) );
parse error:

  syntax error

>>> J = ( (1 / m) * sum(-y'*log(a3)) - (1-y)'*log( 1 - sigmoid(a3))) );
                                                                   ^

>> sum(a3)
ans =

   2500   2500   2500   2500   2500   2500   2500   2500   2500   2500

>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x3, op2 is 1x4)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 78, column 3
>> J =(1 / m) * sum(-y'*log(a3)) - (1-y)'*log(1 - (a3));
error: operator *: nonconformant arguments (op1 is 1x3, op2 is 5000x10)
error: evaluating argument list element number 1
>> size(a1)
ans =

   5000    401

>> size(z2)
ans =

   5000     25

>> size(a2)
ans =

   5000     26

>> ls
checkNNGradients.m		lib
computeNumericalGradient.m	nnCostFunction.m
debugInitializeWeights.m	predict.m
displayData.m			randInitializeWeights.m
ex4.m				sigmoid.m
ex4data1.mat			sigmoidGradient.m
ex4weights.mat			submit.m
fmincg.m
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x3, op2 is 1x4)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 78, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x1, op2 is 4x3)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 78, column 3
>> J =(1 / m) * sum(-y*log(a3)') - (1-y)*log(1 - (a3))';
error: operator *: nonconformant arguments (op1 is 3x1, op2 is 10x5000)
error: evaluating argument list element number 1
>> J =(1 / m) * sum(-y'*log(a3)) - (1-y)'*log(1 - (a3));
error: operator *: nonconformant arguments (op1 is 1x3, op2 is 5000x10)
error: evaluating argument list element number 1
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
>> 
>> a1 = [ones(m,1) X];
error: horizontal dimensions mismatch (5000x1 vs 3x2)
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> a3
a3 =

 Columns 1 through 8:

   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
warning: broken pipe
>> z3
z3 =

   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0
warning: broken pipe
>> 
>> a2
a2 =

 Columns 1 through 8:

   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
   1.00000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000   0.50000
warning: broken pipe
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 2416571 elements using 19332560 bytes

>> y
y =

   4
   2
   3

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   6.7105   7.2415   7.7864   8.3431
   4.8893   5.2734   5.6678   6.0708
   4.1412   4.4516   4.7726   5.1027
   4.5154   4.8618   5.2185   5.5840

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> sum(y_matrix)
ans =

   0   1   1   1   0   0   0   0   0   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x1, op2 is 3x4)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 80, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x4, op2 is 3x4)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 80, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =

   22.260   18.443   26.058

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   7.2236   6.0371   8.4244
   7.6029   6.3412   8.8748
   7.4148   6.1907   8.6509

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   7.5754   6.4479   8.7525

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   7.5754   6.4479   8.7525

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =  22.776
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 80, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =  31.239
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   7.0848   7.5526   8.0446   8.5572

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * (sum(-(y_matrix)'*log(a3)) - (1-y_matrix)'*log(1 - (a3));
                                                                        ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   2.3200   2.4830   2.6529   2.8286
   1.7129   1.8269   1.9467   2.0712
   1.4636   1.5530   1.6482   1.7485
   1.5883   1.6897   1.7969   1.9089

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> 2.32+1.7129+1.4636+1.5883
ans =  7.0848
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
J =

   2.4617   2.0984   2.8423
   2.5882   2.1998   2.9924
   2.5255   2.1497   2.9178

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> J
J =

   2.4617   2.0984   2.8423
   2.5882   2.1998   2.9924
   2.5255   2.1497   2.9178

>> sum(J)
ans =

   7.5754   6.4479   8.7525

>> 7.5754+6.4479 + 8.7525
ans =  22.776
>> 22.776/3
ans =  7.5920
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * (sum (-y_matrix*log(a3)' - (1-y_matrix)*log(1 - (a3))');
                                                                       ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * (sum (-y_matrix'*log(a3) - (1-y_matrix)'*log(1 - (a3)));
                                                                       ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * (sum (-y_matrix'*log(a3) - (1-y_matrix)'*log(1 - (a3));
                                                                      ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * sum (-y_matrix'*log(a3) - (1-y_matrix)'*log(1 - (a3));
                                                                     ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =

   6.7105   7.2415   7.7864   8.3431

grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  30.082
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J = -19.281
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  30.082
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 4x3, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 80, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 80 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = (1 / m) * sum(sum(-y_matrix'*.log(a3) - (1-y_matrix)'*.log(1 - (a3))));
                                    ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 4x3, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 80, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
error: nnCostFunction: product: nonconformant arguments (op1 is 3x2, op2 is 3x6)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 82, column 13
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
warning: operator -: automatic broadcasting operation applied
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x6)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 82, column 13
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator -: nonconformant arguments (op1 is 3x3, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 82, column 13
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator -: nonconformant arguments (op1 is 3x3, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 82, column 13
>> submit()
== Submitting solutions | Neural Networks Learning...
Login (email address): usamah.khan@gmail.com
Token: mYFPLQEE2FrGFMcn
!! Submission failed: unexpected error: operator -: nonconformant arguments (op1 is 16x5, op2 is 16x4)
!! Please try again later.
>> submit()
== Submitting solutions | Neural Networks Learning...
Login (email address): usamah.khan@gmail.com
Token: mYFPLQEE2FrGFMcn
!! Submission failed: unexpected error: operator -: nonconformant arguments (op1 is 16x5, op2 is 16x4)
!! Please try again later.
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator -: nonconformant arguments (op1 is 3x3, op2 is 3x4)
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 82, column 13
>> submit()
== Submitting solutions | Neural Networks Learning...
Login (email address): usamah.khan@gmail.com
Token: mYFPLQEE2FrGFMcn
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   0 /   5 | 
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  30 / 100 | 
== 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
parse error near line 84 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> R = (lambda/2m)* (sum(sum(Theta_1^2)) + sum(sum(Theta_2)^2));
                 ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: 'Theta_1' undefined near line 84 column 28
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 84, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: for A^b, A must be a square matrix. Use .^ for elementwise power.
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 84, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> R = (lambda/2*m)* (sum(sum(Theta1.^2)) + sum(sum(Theta2).^2));
>> R
R = 0
>> R = (lambda/2*m)* (sum(sum(Theta1(2:length(Theta1)).^2)) + sum(sum(Theta2(2:length(Theta2))).^2));
>> R
R = 0
>> Theta1
Theta1 =

 Columns 1 through 20:

   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
!! Submission failed: unexpected error: operator *: nonconformant arguments (op1 is 3x10, op2 is 3x10)
!! Please try again later.
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   0 /   5 | 
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  30 / 100 | 
== 
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   0 /   5 | 
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  30 / 100 | 
== 
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   0 /   5 | 
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  30 / 100 | 
== 
>> g = sigmoid(z)*(1 - sigmoid(z))'
error: 'z' undefined near line 1 column 13
error: evaluating argument list element number 1
>> g = sigmoid(z)*(1 - sigmoid(z))'
error: 'z' undefined near line 1 column 13
error: evaluating argument list element number 1
>> g = sigmoid(0)*(1 - sigmoid(0))'
g =  0.25000
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
!! Submission failed: unexpected error: product: nonconformant arguments (op1 is 3x10, op2 is 10x3)
!! Please try again later.
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  35 / 100 | 
== 
>> rand(1:20)
error: out of memory or dimension too large for Octave's index type
>> rand
ans =  0.62945
>> rand
ans =  0.092236
>> rand*48
ans =  20.987
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        R                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x1                          8  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x10                       240  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 2416615 elements using 19332912 bytes

>> m
m =  5000
>> a1[1]
parse error:

  syntax error

>>> a1[1]
      ^

>> a1(1)
ans =  1
>> a1(2)
ans =  1
>> a1(400)
ans =  1
>> a1(2)
ans =  1
>> a1(2,:)
ans =

 Columns 1 through 8:

   1.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000

 Columns 9 through 16:

   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000

 Columns 17 through 24:

   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000

 Columns 25 through 32:

   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000

 Columns 33 through 40:

   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000   0.00000

 Columns 41 through 48:
>> a1(2,1)
ans =  1
>> a1(2,1:10)
ans =

   1   0   0   0   0   0   0   0   0   0

>> a1(1,1:10)
ans =

   1   0   0   0   0   0   0   0   0   0

>> delta3 = a3 - y_matrix;
error: operator -: nonconformant arguments (op1 is 5000x10, op2 is 3x10)
>> y_matrix
y_matrix =

   0   0   0   1   0   0   0   0   0   0
   0   1   0   0   0   0   0   0   0   0
   0   0   1   0   0   0   0   0   0   0

>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...
parse error near line 23 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/randInitializeWeights.m

  syntax error

>>> W = rand(L out, 1 + L in) * 2 * epsilon init − epsilon init;
                 ^

error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 131, column 16
>> ex4












Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...
parse error near line 23 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/randInitializeWeights.m

  syntax error

>>> W = rand(L_out, 1 + L_in) * 2 * epsilon init − epsilon init;
                                               ^

error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 131, column 16
>> ex4










Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...
error: invalid character '?' (ASCII 226) near line 23, column 47
parse error near line 23 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/randInitializeWeights.m

  syntax error

>>> W = rand(L_out, 1 + L_in) * 2 * epsilon_init − epsilon_init;
                                                 ^

error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 131, column 16
>> ex4










Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -0.00928   0.00000
   0.00890   0.00000
  -0.00836   0.00000
   0.00763   0.00000
  -0.00675   0.00000
  -0.00000   0.00000
   0.00001   0.00000
  -0.00003   0.00000
   0.00004   0.00000
  -0.00005   0.00000
  -0.00018   0.00000
   0.00023   0.00000
  -0.00029   0.00000
   0.00034   0.00000
  -0.00038   0.00000
  -0.00010   0.00000
   0.00012   0.00000
  -0.00014   0.00000
   0.00015   0.00000

Checking Backpropagation (w/ Regularization) ... 
  -0.00928   0.00000
   0.00890   0.00000
  -0.00836   0.00000
   0.00763   0.00000
  -0.00675   0.00000
  -0.00000   0.00000
   0.00001   0.00000
  -0.00003   0.00000
   0.00004   0.00000
  -0.00005   0.00000
  -0.00018   0.00000
   0.00023   0.00000
  -0.00029   0.00000
   0.00034   0.00000
  -0.00038   0.00000
  -0.00010   0.00000
   0.00012   0.00000
  -0.00014   0.00000
   0.00015   0.00000
  -0.00017   0.00000
   0.31454   0.00000

Training Neural Network... 
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.
Qt terminal communication error: select() error 9 Bad file descriptor

Training Set Accuracy: 10.000000
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

>> Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
>                  hidden_layer_size, (input_layer_size + 1));
>> 
>> Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
>                  num_labels, (hidden_layer_size + 1));
>> 
>> % Setup some useful variables
>> m = size(X, 1);
>>          
>> % You need to return the following variables correctly 
>> J = 0;
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
 ESCOD
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2061540 elements using 16492312 bytes

>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 11x1, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double

Total is 56570 elements using 452552 bytes

>> d3 = (a3 - y_matrix);
error: 'a3' undefined near line 1 column 7
>> 
>> d2 = (Theta2(2:end)' * d3) .* sigmoid(z2);
error: 'd3' undefined near line 1 column 23
>> 
>> Delta1 = d2*a1;
error: 'd2' undefined near line 1 column 10
>> Delta2 = d3*a2;
error: 'd3' undefined near line 1 column 10
>> 
>> Theta1_grad = Delta1/m;
error: 'Delta1' undefined near line 1 column 15
>> Theta2_grad = Delta2/m;
error: 'Delta2' undefined near line 1 column 15
>> a1 = [ones(m,1) X];
error: horizontal dimensions mismatch (5000x1 vs 3x2)
>> 
>> z2 = a1*Theta1';
error: 'a1' undefined near line 1 column 6
>> 
>> a2 = sigmoid(z2);
error: 'z2' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> a2 = [ones(m,1) a2];
error: 'a2' undefined near line 1 column 17
>> 
>> z3 = a2*Theta2';
error: 'a2' undefined near line 1 column 6
>> 
>> a3 = sigmoid(z3);
error: 'z3' undefined near line 1 column 14
error: evaluating argument list element number 1
>> m
m =  5000
>> m = size(X, 1);
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
error: operator *: nonconformant arguments (op1 is 3x3, op2 is 401x25)
>> 
>> a2 = sigmoid(z2);
error: 'z2' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> a2 = [ones(m,1) a2];
error: 'a2' undefined near line 1 column 17
>> 
>> z3 = a2*Theta2';
error: 'a2' undefined near line 1 column 6
>> 
>> a3 = sigmoid(z3);Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
>                  hidden_layer_size, (input_layer_size + 1));
error: 'z3' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
>                  num_labels, (hidden_layer_size + 1));
>> 
>> % Setup some useful variables
>> m = size(X, 1);
>>          
>> % You need to return the following variables correctly 
>> J = 0;
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
>                  hidden_layer_size, (input_layer_size + 1));
>> 
>> Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
>                  num_labels, (hidden_layer_size + 1));
>> 
>> % Setup some useful variables
>> m = size(X, 1);
>>          
>> % You need to return the following variables correctly 
>> J = 0;
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
error: operator *: nonconformant arguments (op1 is 3x3, op2 is 401x25)
>> 
>> a2 = sigmoid(z2);
error: 'z2' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> a2 = [ones(m,1) a2];
error: 'a2' undefined near line 1 column 17
>> 
>> z3 = a2*Theta2';
error: 'a2' undefined near line 1 column 6
>> 
>> a3 = sigmoid(z3);
error: 'z3' undefined near line 1 column 14
error: evaluating argument list element number 1
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
>> ex4
warning: broken pipe

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
error: nnCostFunction: operator *: nonconformant arguments (op1 is 259x1, op2 is 5000x10)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        hidden_layer_size      1x1                          8  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2025676 elements using 16205408 bytes

>> ex4












Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -0.00928   0.00000
   0.00890   0.00000
  -0.00836   0.00000
   0.00763   0.00000
  -0.00675   0.00000
  -0.00000   0.00000
   0.00001   0.00000
  -0.00003   0.00000
   0.00004   0.00000
  -0.00005   0.00000
  -0.00018   0.00000
   0.00023   0.00000
  -0.00029   0.00000
   0.00034   0.00000
  -0.00038   0.00000
  -0.00010   0.00000
   0.00012   0.00000
  -0.00014   0.00000
   0.00015   0.00000

Checking Backpropagation (w/ Regularization) ... 
  -0.00928   0.00000
   0.00890   0.00000
  -0.00836   0.00000
   0.00763   0.00000
  -0.00675   0.00000
  -0.00000   0.00000
   0.00001   0.00000
  -0.00003   0.00000
   0.00004   0.00000
  -0.00005   0.00000
  -0.00018   0.00000
   0.00023   0.00000
  -0.00029   0.00000
   0.00034   0.00000
  -0.00038   0.00000
  -0.00010   0.00000
   0.00012   0.00000
  -0.00014   0.00000
   0.00015   0.00000
  -0.00017   0.00000
   0.31454   0.00000

Training Neural Network... 
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero
warning: division by zero

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051255 elements using 16410032 bytes

>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
 ESCOC
                    8  double
                80200  double
                 2080  double
             16000000  double
             16040000  double
              1040000  double
               400000  double
                    8  double
                    0  double
                    0  function_handle
                    8  double
                   40  double
                    8  double
                80200  double
                 2080  double
                82280  double
                    8  double
                    8  double
                    8  double
                82280  double
                    8  double
                    8  struct
                40000  double
                  800  double
                40000  double
               400000  double
              1000000  double
               400000  double

 bytes

>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4461255 elements using 35690032 bytes

>> d3 = (a3 - y_matrix);
>> d2 = (Theta2(2:end)' * d3) .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 259x1, op2 is 5000x10)
>> size(d3)
ans =

   5000     10

>> d2 = (Theta2' * d3) .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 26x10, op2 is 5000x10)
>> d2 = (Theta2 * d3) .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 10x26, op2 is 5000x10)
>> d2 = (Theta2' * d3') .* sigmoid(z2);
error: product: nonconformant arguments (op1 is 26x5000, op2 is 5000x25)
>> d2 = (Theta2 * d3') .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 10x26, op2 is 10x5000)
>> d2 = (Theta2' * d3') .* sigmoid(z2);
error: product: nonconformant arguments (op1 is 26x5000, op2 is 5000x25)
>> d2 = (Theta2' * d3');
>> d2 = (Theta2 * d3');
error: operator *: nonconformant arguments (op1 is 10x26, op2 is 10x5000)
>> d2 = (Theta2' * d3');
>> d2 = (Theta2' * d3') * sigmoid(z2);
>> Delta1 = d2*a1;
error: operator *: nonconformant arguments (op1 is 26x25, op2 is 5000x401)
>> Delta2 = d3*a2;
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 5000x26)
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                    26x25                      5200  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4511906 elements using 36095240 bytes

>> d2 = (Theta2' * d3') * a2;
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                    26x26                      5408  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4511932 elements using 36095448 bytes

>> d2 = d2(2:end)
d2 =

 Columns 1 through 8:

   2114.1   1611.1   1481.3   2382.2   1998.2   1866.8   2906.8   2206.7

 Columns 9 through 16:

   2632.1   1430.1   1836.7   2477.1   2062.6   2351.4   1707.9   1838.6

 Columns 17 through 24:

   2561.4   2188.0   2045.8   1768.9   2682.7   2725.5   2290.9   2061.8

 Columns 25 through 32:

   1897.5   2295.9   2008.9   1531.5   1404.8   2264.2   1900.3   1774.3

 Columns 33 through 40:

   2759.2   2094.3   2502.6   1361.6   1744.1   2353.8   1960.1   2230.8

 Columns 41 through 48:
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                     1x675                     5400  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
>> d2 = d2(:,2:end)
d2 =

 Columns 1 through 8:

   1611.1   1481.3   2382.2   1998.2   1866.8   2906.8   2206.7   2632.1

 Columns 9 through 16:

   1430.1   1836.7   2477.1   2062.6   2351.4   1707.9   1838.6   2561.4

 Columns 17 through 24:

   2188.0   2045.8   1768.9   2682.7   2725.5   2290.9   2061.8   1897.5

 Columns 25 through 32:

   2295.9   2008.9   1531.5   1404.8   2264.2   1900.3   1774.3   2759.2

 Columns 33 through 40:

   2094.3   2502.6   1361.6   1744.1   2353.8   1960.1   2230.8   1621.2

 Columns 41 through 48:
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                     1x674                     5392  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4511930 elements using 36095432 bytes

>> d2 = (Theta2(:,2:end) * d3') * a2;
error: operator *: nonconformant arguments (op1 is 10x25, op2 is 10x5000)
>> d2 = (Theta2(:,2:end)' * d3') * a2;
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                    25x26                      5200  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4511906 elements using 36095240 bytes

>> Delta1 = d2*a1;
error: operator *: nonconformant arguments (op1 is 25x26, op2 is 5000x401)
>> Delta2 = d3*a2;
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 5000x26)
>> Delta2 = d2'*a2;
error: operator *: nonconformant arguments (op1 is 26x25, op2 is 5000x26)
>> Delta2 = d2*a2';
>> Delta1 = d3*a1';
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 401x5000)
>> Delta1 = d3'*a1;
>> Delta1 = d3'*a1;
>> d2 = (Theta2' * d3') * sigmoid(z2);
>> d2 = (Theta2' * d3') .* sigmoid(z2);
error: product: nonconformant arguments (op1 is 26x5000, op2 is 5000x25)
>> d2 = (Theta2' * d3') .* sigmoid(z2)';
error: product: nonconformant arguments (op1 is 26x5000, op2 is 25x5000)
>> d2 = (Theta2(:,2:end)' * d3') .* sigmoid(z2)';
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                10x401                    32080  double
        Delta2                25x5000                 1000000  double
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x2                         16  double
        cost                   0x0                          0  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 4765266 elements using 38122120 bytes

>> d2 = (Theta2(:,2:end)' * d3') .* sigmoid(z2);
error: product: nonconformant arguments (op1 is 25x5000, op2 is 5000x25)
>> Delta1 = d2*a1;
>> Delta2 = d3*a2';
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 26x5000)
>> Delta2 = d3'*a2;
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -1.9349e-02
   8.8991e-03   1.8621e-02
  -8.3601e-03  -1.6844e-02
   7.6281e-03   1.4735e-02
  -6.7480e-03  -1.2913e-02
  -3.0498e-06  -1.1856e-06
   1.4287e-05   3.6158e-05
  -2.5938e-05  -6.1000e-05
   3.6988e-05   7.4713e-05
  -4.6876e-05  -8.7852e-05
  -1.7506e-04  -3.6043e-04
   2.3315e-04   4.8990e-04
  -2.8747e-04  -5.8450e-04
   3.3532e-04   6.5138e-04
  -3.7622e-04  -7.2046e-04
  -9.6266e-05  -2.0330e-04
   1.1798e-04   2.4177e-04
  -1.3715e-04  -2.7060e-04
   1.5325e-04   2.9483e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -1.9349e-02
   8.8991e-03   1.8621e-02
  -8.3601e-03  -1.6844e-02
   7.6281e-03   1.4735e-02
  -6.7480e-03  -1.2913e-02
  -3.0498e-06  -1.1856e-06
   1.4287e-05   3.6158e-05
  -2.5938e-05  -6.1000e-05
   3.6988e-05   7.4713e-05
  -4.6876e-05  -8.7852e-05
  -1.7506e-04  -3.6043e-04
   2.3315e-04   4.8990e-04
  -2.8747e-04  -5.8450e-04
   3.3532e-04   6.5138e-04
  -3.7622e-04  -7.2046e-04
  -9.6266e-05  -2.0330e-04
   1.1798e-04   2.4177e-04
  -1.3715e-04  -2.7060e-04
   1.5325e-04   2.9483e-04
  -1.6656e-04  -3.2088e-04
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration     7 | Cost: 3.256932e+00
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 18.660000
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  35 / 100 | 
== 
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                   5000x400                 16000000  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        hidden_layer_size      1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                   5000x1                      40000  double

Total is 2051260 elements using 16410072 bytes

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
!! Submission failed: unexpected error: operator *: nonconformant arguments (op1 is 1x63, op2 is 16x3)
!! Please try again later.
>> Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
>                  hidden_layer_size, (input_layer_size + 1));
>> 
>> Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
>                  num_labels, (hidden_layer_size + 1));
>> 
>> % Setup some useful variables
>> m = size(X, 1);
>>          
>> % You need to return the following variables correctly 
>> J = 0;
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> 
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> 
>> d3 = (a3 - y_matrix);
>> 
>> d2 = (Theta2(:,2:end)' * d3') .* sigmoid(z2)';
>> 
>> Delta1 = d2(2:end)*a1;
error: operator *: nonconformant arguments (op1 is 1x124999, op2 is 5000x401)
>> Delta2 = d3'*a2;
>> 
>> Theta1_grad = Delta1/m;
error: 'Delta1' undefined near line 1 column 15
>> Theta2_grad = Delta2/m;
>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x5, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 87, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x5, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 87, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0
   0

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 2x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 87, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 1x5, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 2x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   5.06590
   5.27352
  -0.42122
  -0.43858
   0.57029
   0.59421
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 2x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   5.27352
  -0.43858
   0.59421
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x1, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   5.27352
  -0.43858
   0.59421
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x2, op2 is 4x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x2, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x4, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  15x1                        120  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 2641850 elements using 21134792 bytes

>> Thetatest = Theta2(:,2:end);
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  15x1                        120  double
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x2, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 2x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x2, op2 is 4x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 2x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 89, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 89, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   3.43259
   4.54520
   0.43100
   0.56792
   5.41856
   7.17722
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   6.69821
   8.87280
   0.58382
   0.77212
  -0.76914
  -1.01754
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x2, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
  -0.33574
   1.37851
   0.58621
  -0.42287
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   0.12058
   0.24165
   2.32958
   0.22910
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
  -0.33574
   1.37851
   0.58621
  -0.42287
   1.93364
   0.60637
   0.51004
   0.69891
   0.44798
   0.39686
   0.49347
   0.44950
   0.41101
   0.48985
   0.30584
   0.20445
   0.40335

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 89, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
  -0.33574
   1.37851
   0.58621
  -0.42287
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
  -0.33574
   1.37851
   0.58621
  -0.42287
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 2x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   1.68863
   2.32958
   0.12058
   0.22910
   0.24165
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   2.32958
   0.22910
   0.38742
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x1, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 88, column 9
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.58621
  -0.42287
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x3, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x3, op2 is 4x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 4x4, op2 is 3x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 3x4, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 4x4, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: operator *: nonconformant arguments (op1 is 4x2, op2 is 3x4)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
error: nnCostFunction: product: nonconformant arguments (op1 is 3x2, op2 is 3x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 85, column 5
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.00000
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.44163
   0.58621
  -0.33574
  -0.42287
   1.37851
   1.93364
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta1_grad           25x401                    80200  double
        Theta2                10x26                      2080  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                  5000x401                 16040000  double
        a2                  5000x26                   1040000  double
        a3                  5000x10                    400000  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix            5000x10                    400000  double
        z2                  5000x25                   1000000  double
        z3                  5000x10                    400000  double

Total is 2642103 elements using 21136816 bytes

>> a1 = [ones(m,1) X];
error: horizontal dimensions mismatch (5000x1 vs 3x2)
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> m = size(X, 1);
>> a1 = [ones(m,1) X];
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...
>                  hidden_layer_size, (input_layer_size + 1));
>> 
>> Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...
>                  num_labels, (hidden_layer_size + 1));
>> Theta1 = reshape(nn(1:hl * (il + 1)), ...
>                  hl, (il + 1));
>> 
>> Theta2 = reshape(nn((1 + (hl * (il + 1))):end), ...
>                  nl, (hl + 1));
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> 
>> 
>> d3 = (a3 - y_matrix);
error: operator -: nonconformant arguments (op1 is 3x4, op2 is 3x10)
>> 
>> d2 = (d3*Theta2(:,2:end)) .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 4x2)
>> 
>> 
>> Delta1 = d2'*a1';
error: operator *: nonconformant arguments (op1 is 5000x25, op2 is 3x3)
>> Delta2 = d3'*a2;
error: operator *: nonconformant arguments (op1 is 10x5000, op2 is 3x3)
>> 
>> Theta1_grad = Delta1/m;
error: 'Delta1' undefined near line 1 column 15
>> Theta2_grad = Delta2/m;
>> d3
d3 =

 Columns 1 through 7:

   0.082317   0.093801   0.098607   0.095679   0.096240   0.094060   0.094610
   0.079445   0.090722   0.095507   0.092541   0.092847   0.090763   0.091362
   0.079886   0.090926   0.095793   0.092948   0.093270   0.091284   0.091771
   0.079168   0.090307   0.095183   0.092255   0.092524   0.090518   0.090996
   0.075824   0.086837   0.091654   0.088639   0.088717   0.086830   0.087392
   0.076492   0.087616   0.092496   0.089346   0.089547   0.087599   0.088061
   0.075775   0.086695   0.091556   0.088550   0.088612   0.086752   0.087301
   0.085950   0.098237   0.102902   0.100037   0.100233   0.097619   0.097566
   0.077192   0.088228   0.093079   0.090073   0.090283   0.088291   0.088808
   0.080848   0.092209   0.097021   0.094130   0.094457   0.092141   0.092815
   0.079405   0.090667   0.095485   0.092537   0.092873   0.090746   0.091359
   0.079471   0.091050   0.095821   0.092677   0.093007   0.090706   0.091229
   0.079150   0.090441   0.095274   0.092254   0.092707   0.090552   0.091052
   0.082225   0.093870   0.098766   0.095780   0.095791   0.093580   0.093868
   0.077061   0.088087   0.092994   0.089908   0.090066   0.088167   0.088657
   0.087850   0.099949   0.104537   0.101654   0.102557   0.099944   0.100355
   0.076522   0.087525   0.092396   0.089359   0.089569   0.087664   0.088131
   0.075524   0.086469   0.091402   0.088219   0.088350   0.086510   0.086935
   0.085235   0.096884   0.101614   0.098746   0.099319   0.097073   0.097389
warning: broken pipe
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad           25x401                    80200  double
        Theta2                 4x3                         96  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
 ESCOC


                Bytes  Class
                =====  ===== 
                 2080  double
                    8  double
                   48  double
                80200  double
                   96  double
                 2080  double
                 2000  double
                   48  double
                   72  double
                   72  double
                   96  double
                    8  double
                   40  double
                    0  function_handle
              1000000  double
               400000  double
                    8  double
                   40  double
                  144  double
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad           25x401                    80200  double
        Theta2                 4x3                         96  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x10                       240  double
        z2                     3x2                         48  double
        z3                     3x4                         96  double

Total is 221914 elements using 1775304 bytes

>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> d3 = (a3 - y_matrix);
error: operator -: nonconformant arguments (op1 is 3x4, op2 is 3x10)
>> 
>> d2 = (d3*Theta2(:,2:end)) .* sigmoid(z2);
error: operator *: nonconformant arguments (op1 is 5000x10, op2 is 4x2)
>> 
>> 
>> Delta1 = d2'*a1';
error: operator *: nonconformant arguments (op1 is 5000x25, op2 is 3x3)
>> Delta2 = d3'*a2;
error: operator *: nonconformant arguments (op1 is 10x5000, op2 is 3x3)
>> 
>> Theta1_grad = Delta1/m;
error: 'Delta1' undefined near line 1 column 15
>> Theta2_grad = Delta2/m;
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta2                10x26                      2080  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad           25x401                    80200  double
        Theta2                 4x3                         96  double
        Theta2_grad           10x26                      2080  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                    25x5000                 1000000  double
        d3                  5000x10                    400000  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x10                       240  double
        z2                     3x2                         48  double
        z3                     3x4                         96  double

Total is 221914 elements using 1775304 bytes

>> y_matrix = eye(num_labels)(y,:) ;
>> y_matrix = eye(nl)(y,:) ;
>> d3 = (a3 - y_matrix);
>> 
>> d2 = (d3*Theta2(:,2:end)) .* sigmoid(z2);
>> 
>> 
>> Delta1 = d2'*a1';
>> Delta2 = d3'*a2;
>> 
>> Theta1_grad = Delta1/m;
>> Theta2_grad = Delta2/m;
>> 
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                 2x3                         48  double
        Delta2                 4x3                         96  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad            2x3                         48  double
        Theta2                 4x3                         96  double
        Theta2_grad            4x3                         96  double
        Thetatest             10x25                      2000  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                   5x1                         40  double
        costFunction           1x1                          0  function_handle
        d2                     3x2                         48  double
        d3                     3x4                         96  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x4                         96  double
        z2                     3x2                         48  double
        z3                     3x4                         96  double

Total is 36405 elements using 291232 bytes

>> d2
d2 =

   1.6319   2.2963
   1.1731   1.4796
   2.2609   3.2129

>> d3
d3 =

   0.888659   0.907427   0.923305  -0.063351
   0.838178  -0.139718   0.879800   0.896918
   0.923414   0.938578  -0.049102   0.960851

>> z2
z2 =

   0.054017   0.166433
  -0.523820  -0.588183
   0.665184   0.889567

>> Delta2
Delta2 =

   2.65025   1.37794   1.43501
   1.70629   1.03385   1.10676
   1.75400   0.76894   0.77931
   1.79442   0.93566   0.96699

>> d2 = (d3*Theta2(:,2:end)) .* sigmoidGradient(z2);
>> d2
d2 =

   0.79393   1.05281
   0.73674   0.95128
   0.76775   0.93560

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.29083
   0.39248
  -0.14575
  -0.16683
   0.58003
   0.74033
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, 0)
J =  7.4070
grad =

   0.766138
   0.979897
  -0.027540
  -0.035844
  -0.024929
  -0.053862
   0.883417
   0.568762
   0.584668
   0.598139
   0.459314
   0.344618
   0.256313
   0.311885
   0.478337
   0.368920
   0.259771
   0.322331

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> ComputeNumericalGradient
error: 'ComputeNumericalGradient' undefined near line 1 column 1
>> ComputeNumericalGradient()
error: 'ComputeNumericalGradient' undefined near line 1 column 1
>> CheckNNGradients()
error: 'CheckNNGradients' undefined near line 1 column 1
>> CheckNNGradients(lambda)
error: 'CheckNNGradients' undefined near line 1 column 1
>> checkNNGradients(lambda)
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
>> checkNNGradients(lambda)
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
   1.6409e-01   1.6409e-01
   5.7574e-02   5.7574e-02
   5.0458e-02   5.0458e-02
   1.6457e-01   1.6457e-01
   5.7787e-02   5.7787e-02
   5.0753e-02   5.0753e-02
   1.5834e-01   1.5834e-01
   5.5924e-02   5.5924e-02
   4.9162e-02   4.9162e-02
   1.5113e-01   1.5113e-01
   5.3697e-02   5.3697e-02
   4.7146e-02   4.7146e-02
   1.4957e-01   1.4957e-01
   5.3154e-02   5.3154e-02
   4.6560e-02   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: 2.32978e-11
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
   1.6409e-01   1.6409e-01
   5.7574e-02   5.7574e-02
   5.0458e-02   5.0458e-02
   1.6457e-01   1.6457e-01
   5.7787e-02   5.7787e-02
   5.0753e-02   5.0753e-02
   1.5834e-01   1.5834e-01
   5.5924e-02   5.5924e-02
   4.9162e-02   4.9162e-02
   1.5113e-01   1.5113e-01
   5.3697e-02   5.3697e-02
   4.7146e-02   4.7146e-02
   1.4957e-01   1.4957e-01
   5.3154e-02   5.3154e-02
   4.6560e-02   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: 2.32978e-11

Program paused. Press enter to continue.

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
   1.6409e-01   1.6409e-01
   5.7574e-02   5.7574e-02
   5.0458e-02   5.0458e-02
   1.6457e-01   1.6457e-01
   5.7787e-02   5.7787e-02
   5.0753e-02   5.0753e-02
   1.5834e-01   1.5834e-01
   5.5924e-02   5.5924e-02
   4.9162e-02   4.9162e-02
   1.5113e-01   1.5113e-01
   5.3697e-02   5.3697e-02
   4.7146e-02   4.7146e-02
   1.4957e-01   1.4957e-01
   5.3154e-02   5.3154e-02
   4.6560e-02   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: 2.32978e-11


Cost at (fixed) debugging parameters (w/ lambda = 10): 0.287629 
(this value should be about 0.576051)

Program paused. Press enter to continue.

Training Neural Network... 
Iteration    50 | Cost: 6.175724e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 91.820000
>> load('ex4data1.mat');
>> m = size(X, 1);
>> 
>> % Randomly select 100 data points to display
>> sel = randperm(size(X, 1));
>> sel = sel(1:100);
>> 
>> displayData(X(sel, :));
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 7.509393e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 89.340000
>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I,J,...) = X: dimensions mismatch
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.766138
   0.979897
  -0.027540
  -0.035844
  -0.024929
  -0.053862
   0.883417
   0.568762
   0.584668
   0.598139
   0.459314
   0.344618
   0.256313
   0.311885
   0.478337
   0.368920
   0.259771
   0.322331

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   3.064553
   3.919587
  -0.110158
  -0.143377
  -0.099715
  -0.215447
   3.533669
   2.275049
   2.338671
   2.392557
   1.837254
   1.378473
   1.025253
   1.247540
   1.913346
   1.475682
   1.039082
   1.289324

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I,J,...) = X: dimensions mismatch
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I) = X: X must have the same size as I
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 21
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I,J,...) = X: dimensions mismatch
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I) = X: X must have the same size as I
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 21
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   3.064553
   3.919587
  -0.110158
  -0.143377
  -0.099715
  -0.215447
   0.883417
   0.568762
   0.584668
   0.598139
   0.459314
   0.344618
   0.256313
   0.311885
   0.478337
   0.368920
   0.259771
   0.322331

>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                      3x2                         48  double
        ans                    1x1                          8  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double

Total is 46353 elements using 370816 bytes

>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
error: horizontal dimensions mismatch (5000x1 vs 3x2)
>> 
>> z2 = a1*Theta1';
error: 'a1' undefined near line 1 column 6
>> 
>> a2 = sigmoid(z2);
error: 'z2' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> a2 = [ones(m,1) a2];
error: 'a2' undefined near line 1 column 17
>> 
>> z3 = a2*Theta2';
error: 'a2' undefined near line 1 column 6
>> 
>> a3 = sigmoid(z3);
error: 'z3' undefined near line 1 column 14
error: evaluating argument list element number 1
>> m = 3
m =  3
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
error: operator *: nonconformant arguments (op1 is 3x3, op2 is 401x25)
>> 
>> a2 = sigmoid(z2);
error: 'z2' undefined near line 1 column 14
error: evaluating argument list element number 1
>> 
>> a2 = [ones(m,1) a2];
error: 'a2' undefined near line 1 column 17
>> 
>> z3 = a2*Theta2';
error: 'a2' undefined near line 1 column 6
>> 
>> a3 = sigmoid(z3);
error: 'z3' undefined near line 1 column 14
error: evaluating argument list element number 1
>> %Theta1 = reshape(nn(1:hl * (il + 1)), ...
>> %                 hl, (il + 1));
>> 
>> %Theta2 = reshape(nn((1 + (hl * (il + 1))):end), ...
>> %                 nl, (hl + 1));
>> Theta1 = reshape(nn(1:hl * (il + 1)), ...
>                  hl, (il + 1));
>> 
>> Theta2 = reshape(nn((1 + (hl * (il + 1))):end), ...
>                  nl, (hl + 1));
>> Theta1_grad = zeros(size(Theta1));
>> Theta2_grad = zeros(size(Theta2));
>> y_matrix = eye(num_labels)(y,:) ;
>> 
>> a1 = [ones(m,1) X];
>> 
>> z2 = a1*Theta1';
>> 
>> a2 = sigmoid(z2);
>> 
>> a2 = [ones(m,1) a2];
>> 
>> z3 = a2*Theta2';
>> 
>> a3 = sigmoid(z3);
>> 
>> d3 = (a3 - y_matrix);
error: operator -: nonconformant arguments (op1 is 3x4, op2 is 3x10)
>> 
>> d2 = (d3*Theta2(:,2:end)) .* sigmoidGradient(z2);
error: 'd3' undefined near line 1 column 7
>> 
>> 
>> Delta1 = d2'*a1;
error: 'd2' undefined near line 1 column 10
>> Delta2 = d3'*a2;
error: 'd3' undefined near line 1 column 10
>> 
>> Theta1_grad = Delta1/m;
error: 'Delta1' undefined near line 1 column 15
>> Theta2_grad = Delta2/m;
error: 'Delta2' undefined near line 1 column 15
>> y_matrix = eye(nl)(y,:) ;
>> d3 = (a3 - y_matrix);
>> 
>> d2 = (d3*Theta2(:,2:end)) .* sigmoidGradient(z2);
>> 
>> 
>> Delta1 = d2'*a1;
>> Delta2 = d3'*a2;
>> 
>> Theta1_grad = Delta1/m;
>> Theta2_grad = Delta2/m;
>> 
>> Theta1_grad = Delta1*(lambda/m);
>> %Theta2_grad(:,2:end) = Delta2*(lambda/m);
>> Theta1_grad = Delta1/m;
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                 2x3                         48  double
        Delta2                 4x3                         96  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad            2x3                         48  double
        Theta2                 4x3                         96  double
        Theta2_grad            4x3                         96  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        d2                     3x2                         48  double
        d3                     3x4                         96  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x4                         96  double
        z2                     3x2                         48  double
        z3                     3x4                         96  double

Total is 36200 elements using 289592 bytes

>> Theta1_grad
Theta1_grad =

   0.766138  -0.027540  -0.024929
   0.979897  -0.035844  -0.053862

>> Theta1_grad(:,2:end)
ans =

  -0.027540  -0.024929
  -0.035844  -0.053862

>> Theta1_grad(:,2:end)
ans =

  -0.027540  -0.024929
  -0.035844  -0.053862

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: operator +: nonconformant arguments (op1 is 2x3, op2 is 2x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 91, column 14
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.766138
   0.979897
  -0.027540
  -0.035844
  -0.024929
  -0.053862
   0.883417
   0.568762
   0.584668
   0.598139
   0.459314
   0.344618
   0.256313
   0.311885
   0.478337
   0.368920
   0.259771
   0.322331

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 94 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> 	Theta1_grad(:,2:end) = Delta1(:,2:end)/m + (lambda/m)*Theta1(:,2:end));
                                                                          ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 94 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> 	Theta1_grad(:,2:end) = Delta1(2:end)/m + (lambda/m)*Theta1(:,2:end));
                                                                        ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 94 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> 	Theta1_grad(:,2:end) = Delta1/m + (lambda/m)*Theta1(:,2:end));
                                                                 ^

>> Delta1/m
ans =

   0.766138  -0.027540  -0.024929
   0.979897  -0.035844  -0.053862

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 94 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> 	Theta1_grad(:,2:end) = Delta1(2:end)/m + (lambda/m)*Theta1);
                                                               ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: operator +: nonconformant arguments (op1 is 1x5, op2 is 2x3)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I,J,...) = X: dimensions mismatch
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: operator +: nonconformant arguments (op1 is 2x3, op2 is 2x2)
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 94, column 23
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   0.45931
   0.34462
   0.25631
   0.31189
   0.47834
   0.36892
   0.25977
   0.32233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                 2x3                         48  double
        Delta2                 4x3                         96  double
        J                      1x1                          8  double
        Theta1                 2x3                         48  double
        Theta1_grad            2x3                         48  double
        Theta2                 4x3                         96  double
        Theta2_grad            4x3                         96  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    2x3                         48  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        d2                     3x2                         48  double
        d3                     3x4                         96  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
>> sum(grad)
ans =  24.907
>> sum(grad)/2 + 7.40
ans =  19.854
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =

    9.5975   11.8613   13.2156

grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
>> whos
Variables in the current scope:
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        d2                     3x2                         48  double
        d3                     3x4                         96  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                 2x3                         48  double
        Delta2                 4x3                         96  double
        J                      1x3                         24  double
        Theta1                 2x3                         48  double
        Theta1_grad            2x3                         48  double
        Theta2                 4x3                         96  double
        Theta2_grad            4x3                         96  double
        X                      3x2                         48  double
        nn_params          10285x1                      82280  double
>> 
>>      a3                     3x4                         96  double
>>      ans                    1x1                          8  double
>>      cost                  50x1                        400  double
>>      costFunction           1x1                          0  function_handle
>>      d2                     3x2                         48  double
>>      d3                     3x4                         96  double
>>      debug_J                1x1                          8  double
>>      g                      1x5                         40  double
>>      grad                  18x1                        144  double
>>      hidden_layer_size      1x1                          8  double
>>      hl                     1x1                          8  double
>>      il                     1x1                          8  double
>>      initial_Theta1        25x401                    80200  double
>>      initial_Theta2        10x26                      2080  double
>>      initial_nn_params  10285x1                      82280  double
>>      input_layer_size       1x1                          8  double
>>      lambda                 1x1                          8  double
>>      m                      1x1                          8  double
>>      nl                     1x1                          8  double
>>      nn                     1x18                       144  double
>>      nn_params          10285x1                      82280  double
>> 
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        Delta1                 2x3                         48  double
        Delta2                 4x3                         96  double
        J                      1x3                         24  double
        Theta1                 2x3                         48  double
        Theta1_grad            2x3                         48  double
        Theta2                 4x3                         96  double
        Theta2_grad            4x3                         96  double
        X                      3x2                         48  double
        a1                     3x3                         72  double
        a2                     3x3                         72  double
        a3                     3x4                         96  double
        ans                    1x1                          8  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        d2                     3x2                         48  double
        d3                     3x4                         96  double
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double
        y_matrix               3x4                         96  double
        z2                     3x2                         48  double
        z3                     3x4                         96  double

Total is 36202 elements using 289608 bytes

>>  (sum(Theta1_grad) + sum(Theta2_grad))/2
ans =

   2.19051   0.65437   0.67528

>> sum(Theta1_grad)
ans =

   1.746035  -0.063384  -0.078790

>>  sum((sum(Theta1_grad) + sum(Theta2_grad)))/2
ans =  3.5202
>>  sum((sum(Theta1_grad) + sum(Theta2_grad)))/2
ans =  3.5202
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.860
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
!! Submission failed: unexpected error: operator +: nonconformant arguments (op1 is 1x3, op2 is 1x5)
!! Please try again later.
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
error: nnCostFunction: operator +: nonconformant arguments (op1 is 1x401, op2 is 1x26)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 98, column 3
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> 
>> ex4






























Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.000000 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.000000 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   0.00000  -0.00928
   0.00000   0.00890
   0.00000  -0.00836
   0.00000   0.00763
   0.00000  -0.00675
   0.00000  -0.00000
   0.00000   0.00001
   0.00000  -0.00003
   0.00000   0.00004
   0.00000  -0.00005
   0.00000  -0.00018
   0.00000   0.00023
   0.00000  -0.00029
   0.00000   0.00034
   0.00000  -0.00038
   0.00000  -0.00010
   0.00000   0.00012
   0.00000  -0.00014
   0.00000   0.00015

Checking Backpropagation (w/ Regularization) ... 
   0.00000  -0.00928
   0.00000   0.00890
   0.00000  -0.00836
   0.00000   0.00763
   0.00000  -0.00675
   0.00000  -0.00000
   0.00000   0.00001
   0.00000  -0.00003
   0.00000   0.00004
   0.00000  -0.00005
   0.00000  -0.00018
   0.00000   0.00023
   0.00000  -0.00029
   0.00000   0.00034
   0.00000  -0.00038
   0.00000  -0.00010
   0.00000   0.00012
   0.00000  -0.00014
   0.00000   0.00015
   0.00000  -0.00017
   0.00000   0.31454

Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 6.799931e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 91.100000
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: 'nn' undefined near line 1 column 26
error: evaluating argument list element number 1
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 7.255581e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 89.980000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
error: nnCostFunction: operator +: nonconformant arguments (op1 is 1x401, op2 is 1x26)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 98, column 3
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> ex4































Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
error: nnCostFunction: operator +: nonconformant arguments (op1 is 1x401, op2 is 1x26)
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 98, column 3
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> ex4































Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -1.6768e-02
   1.4287e-05   3.9433e-02
  -2.5938e-05   5.9336e-02
   3.6988e-05   2.4764e-02
  -4.6876e-05  -3.2688e-02
  -1.7506e-04  -6.0174e-02
   2.3315e-04  -3.1961e-02
  -2.8747e-04   2.4923e-02
   3.3532e-04   5.9772e-02
  -3.7622e-04   3.8641e-02
  -9.6266e-05  -1.7370e-02
   1.1798e-04  -5.7566e-02
  -1.3715e-04  -4.5196e-02
   1.5325e-04   9.1459e-03
  -1.6656e-04   5.4610e-02
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 7.888552e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 89.880000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
parse error near line 99 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = J + sum(sum(Theta1_grad)) + sum(sum(Theta2_grad))) ;
                                                         ^

error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/ex4.m at line 77, column 3
>> ex4




























Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.313677 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.297585 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   7.8271e-02  -9.2783e-03
   1.8371e-01   8.8991e-03
   7.7141e-02  -8.3601e-03
   1.8332e-01   7.6281e-03
   7.9913e-02  -6.7480e-03
   3.0436e-04  -3.0498e-06
   6.2562e-04   1.4287e-05
   2.7848e-04  -2.5938e-05
   6.6466e-04   3.6988e-05
   2.5145e-04  -4.6876e-05
   1.9877e-03  -1.7506e-04
   4.5487e-03   2.3315e-04
   1.8289e-03  -2.8747e-04
   4.6831e-03   3.3532e-04
   1.7544e-03  -3.7622e-04
   8.2330e-04  -9.6266e-05
   1.9550e-03   1.1798e-04
   7.5911e-04  -1.3715e-04
   1.9922e-03   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
   7.8271e-02  -9.2783e-03
   1.8371e-01   8.8991e-03
   7.7141e-02  -8.3601e-03
   1.8332e-01   7.6281e-03
   7.9913e-02  -6.7480e-03
   6.0030e-01  -1.6768e-02
   6.0063e-01   3.9433e-02
   6.0028e-01   5.9336e-02
   6.0066e-01   2.4764e-02
   6.0025e-01  -3.2688e-02
   6.0199e-01  -6.0174e-02
   6.0455e-01  -3.1961e-02
   6.0183e-01   2.4923e-02
   6.0468e-01   5.9772e-02
   6.0175e-01   3.8641e-02
   6.0082e-01  -1.7370e-02
   6.0195e-01  -5.7566e-02
   6.0076e-01  -4.5196e-02
   6.0199e-01   9.1459e-03
   6.0074e-01   5.4610e-02
   1.1866e+00   3.1454e-01

Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.313677 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.297585 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   7.8271e-02  -9.2783e-03
   1.8371e-01   8.8991e-03
   7.7141e-02  -8.3601e-03
   1.8332e-01   7.6281e-03
   7.9913e-02  -6.7480e-03
   3.0436e-04  -3.0498e-06
   6.2562e-04   1.4287e-05
   2.7848e-04  -2.5938e-05
   6.6466e-04   3.6988e-05
   2.5145e-04  -4.6876e-05
   1.9877e-03  -1.7506e-04
   4.5487e-03   2.3315e-04
   1.8289e-03  -2.8747e-04
   4.6831e-03   3.3532e-04
   1.7544e-03  -3.7622e-04
   8.2330e-04  -9.6266e-05
   1.9550e-03   1.1798e-04
   7.5911e-04  -1.3715e-04
   1.9922e-03   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
   7.8271e-02  -9.2783e-03
   1.8371e-01   8.8991e-03
   7.7141e-02  -8.3601e-03
   1.8332e-01   7.6281e-03
   7.9913e-02  -6.7480e-03
   6.0030e-01  -1.6768e-02
   6.0063e-01   3.9433e-02
   6.0028e-01   5.9336e-02
   6.0066e-01   2.4764e-02
   6.0025e-01  -3.2688e-02
   6.0199e-01  -6.0174e-02
   6.0455e-01  -3.1961e-02
   6.0183e-01   2.4923e-02
   6.0468e-01   5.9772e-02
   6.0175e-01   3.8641e-02
   6.0082e-01  -1.7370e-02
   6.0195e-01  -5.7566e-02
   6.0076e-01  -4.5196e-02
   6.0199e-01   9.1459e-03
   6.0074e-01   5.4610e-02
   1.1866e+00   3.1454e-01
   9.8068e-01   1.1106e-01


Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
   1.6409e-01   1.6409e-01
   5.7574e-02   5.7574e-02
   5.0458e-02   5.0458e-02
   1.6457e-01   1.6457e-01
   5.7787e-02   5.7787e-02
   5.0753e-02   5.0753e-02
   1.5834e-01   1.5834e-01
   5.5924e-02   5.5924e-02
   4.9162e-02   4.9162e-02
   1.5113e-01   1.5113e-01
   5.3697e-02   5.3697e-02
   4.7146e-02   4.7146e-02
   1.4957e-01   1.4957e-01
   5.3154e-02   5.3154e-02
   4.6560e-02   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: 2.32978e-11

Program paused. Press enter to continue.

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -1.6768e-02
   1.4287e-05   3.9433e-02
  -2.5938e-05   5.9336e-02
   3.6988e-05   2.4764e-02
  -4.6876e-05  -3.2688e-02
  -1.7506e-04  -6.0174e-02
   2.3315e-04  -3.1961e-02
  -2.8747e-04   2.4923e-02
   3.3532e-04   5.9772e-02
  -3.7622e-04   3.8641e-02
  -9.6266e-05  -1.7370e-02
   1.1798e-04  -5.7566e-02
  -1.3715e-04  -4.5196e-02
   1.5325e-04   9.1459e-03
  -1.6656e-04   5.4610e-02
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 7.109293e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 91.060000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.300653 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.292607 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   3.4496e-02  -9.2783e-03
   9.6306e-02   8.8991e-03
   3.4390e-02  -8.3601e-03
   9.5472e-02   7.6281e-03
   3.6582e-02  -6.7480e-03
   1.5065e-04  -3.0498e-06
   3.1996e-04   1.4287e-05
   1.2627e-04  -2.5938e-05
   3.5083e-04   3.6988e-05
   1.0229e-04  -4.6876e-05
   9.0631e-04  -1.7506e-04
   2.3909e-03   2.3315e-04
   7.7073e-04  -2.8747e-04
   2.5092e-03   3.3532e-04
   6.8909e-04  -3.7622e-04
   3.6351e-04  -9.6266e-05
   1.0365e-03   1.1798e-04
   3.1098e-04  -1.3715e-04
   1.0727e-03   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
   3.4496e-02  -9.2783e-03
   9.6306e-02   8.8991e-03
   3.4390e-02  -8.3601e-03
   9.5472e-02   7.6281e-03
   3.6582e-02  -6.7480e-03
   3.0015e-01  -1.6768e-02
   3.0032e-01   3.9433e-02
   3.0013e-01   5.9336e-02
   3.0035e-01   2.4764e-02
   3.0010e-01  -3.2688e-02
   3.0091e-01  -6.0174e-02
   3.0239e-01  -3.1961e-02
   3.0077e-01   2.4923e-02
   3.0251e-01   5.9772e-02
   3.0069e-01   3.8641e-02
   3.0036e-01  -1.7370e-02
   3.0104e-01  -5.7566e-02
   3.0031e-01  -4.5196e-02
   3.0107e-01   9.1459e-03
   3.0029e-01   5.4610e-02
   7.5059e-01   3.1454e-01

Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.860
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.860
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.860
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: for A^b, A must be a square matrix. Use .^ for elementwise power.
error: evaluating argument list element number 1
error: evaluating argument list element number 1
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 100, column 3
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  30.956
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  54.506
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  38.806
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  15.257
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  30.956
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  54.506
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  54.506
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  460.38
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  913.35
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  2725.2
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  309.39
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  82.902
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  189.34
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  37.730
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  11.558
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  24.012
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  24.012
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =

   7.7900
   7.8969
   7.5932
   7.6557
   7.7278
   7.7800
   7.8487
   7.6914
   7.6993
   7.7060
   8.3700
   8.3793
   8.4018
   8.4962
   8.6461
   8.6581
   8.6702
   8.7681

grad =

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 100 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> J = J + (sum(sum(Theta1)) + sum(sum(Theta2)))*(lambda/(2*m);
                                                               ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.507
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  21.770
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.507
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 9.530273 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 9.520265 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -3.0498e-06
   1.0000e+00   1.4287e-05
   9.9997e-01  -2.5938e-05
   1.0000e+00   3.6988e-05
   9.9995e-01  -4.6876e-05
   9.9982e-01  -1.7506e-04
   1.0002e+00   2.3315e-04
   9.9971e-01  -2.8747e-04
   1.0003e+00   3.3532e-04
   9.9962e-01  -3.7622e-04
   9.9990e-01  -9.6266e-05
   1.0001e+00   1.1798e-04
   9.9986e-01  -1.3715e-04
   1.0002e+00   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -1.6768e-02
   1.0000e+00   3.9433e-02
   9.9997e-01   5.9336e-02
   1.0000e+00   2.4764e-02
   9.9995e-01  -3.2688e-02
   9.9982e-01  -6.0174e-02
   1.0002e+00  -3.1961e-02
   9.9971e-01   2.4923e-02
   1.0003e+00   5.9772e-02
   9.9962e-01   3.8641e-02
   9.9990e-01  -1.7370e-02
   1.0001e+00  -5.7566e-02
   9.9986e-01  -4.5196e-02
   1.0002e+00   9.1459e-03
   9.9983e-01   5.4610e-02
   6.1454e-01   3.1454e-01


Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 9.530273 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 9.520265 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -3.0498e-06
   1.0000e+00   1.4287e-05
   9.9997e-01  -2.5938e-05
   1.0000e+00   3.6988e-05
   9.9995e-01  -4.6876e-05
   9.9982e-01  -1.7506e-04
   1.0002e+00   2.3315e-04
   9.9971e-01  -2.8747e-04
   1.0003e+00   3.3532e-04
   9.9962e-01  -3.7622e-04
   9.9990e-01  -9.6266e-05
   1.0001e+00   1.1798e-04
   9.9986e-01  -1.3715e-04
   1.0002e+00   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -1.6768e-02
   1.0000e+00   3.9433e-02
   9.9997e-01   5.9336e-02
   1.0000e+00   2.4764e-02
   9.9995e-01  -3.2688e-02
   9.9982e-01  -6.0174e-02
   1.0002e+00  -3.1961e-02
   9.9971e-01   2.4923e-02
   1.0003e+00   5.9772e-02
   9.9962e-01   3.8641e-02
   9.9990e-01  -1.7370e-02
   1.0001e+00  -5.7566e-02
   9.9986e-01  -4.5196e-02
   1.0002e+00   9.1459e-03
   9.9983e-01   5.4610e-02
   6.1454e-01   3.1454e-01

Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> checkNNGradients(lambda)
   0.9907217  -0.0092783
   1.0088991   0.0088991
   0.9916399  -0.0083601
   1.0076281   0.0076281
   0.9932520  -0.0067480
   0.9999970  -0.0055914
   1.0000143   0.0131540
   0.9999741   0.0197612
   1.0000370   0.0082794
   0.9999531  -0.0109273
   0.9998249  -0.0201749
   1.0002331  -0.0104983
   0.9997125   0.0081159
   1.0003353   0.0201475
   0.9996238   0.0126295
   0.9999037  -0.0058543
   1.0001180  -0.0191100
   0.9998629  -0.0151569
   1.0001532   0.0031508
   0.9998334   0.0180923
   0.4145450   0.3145450
   0.2110566   0.1110566
   0.1974007   0.0974007
>> checkNNGradients(lambda)
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04
  -1.6656e-04  -1.6656e-04
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 7.536419e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 90.260000
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 9.530273 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 9.520265 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -3.0498e-06
   1.0000e+00   1.4287e-05
   9.9997e-01  -2.5938e-05
   1.0000e+00   3.6988e-05
   9.9995e-01  -4.6876e-05
   9.9982e-01  -1.7506e-04
   1.0002e+00   2.3315e-04
   9.9971e-01  -2.8747e-04
   1.0003e+00   3.3532e-04
   9.9962e-01  -3.7622e-04
   9.9990e-01  -9.6266e-05
   1.0001e+00   1.1798e-04
   9.9986e-01  -1.3715e-04
   1.0002e+00   1.5325e-04
   9.9983e-01  -1.6656e-04
   3.1454e-01   3.1454e-01
   1.1106e-01   1.1106e-01
   9.7401e-02   9.7401e-02
   1.6409e-01   1.6409e-01
   5.7574e-02   5.7574e-02
   5.0458e-02   5.0458e-02
   1.6457e-01   1.6457e-01
   5.7787e-02   5.7787e-02
   5.0753e-02   5.0753e-02
   1.5834e-01   1.5834e-01
   5.5924e-02   5.5924e-02
   4.9162e-02   4.9162e-02
   1.5113e-01   1.5113e-01
   5.3697e-02   5.3697e-02
   4.7146e-02   4.7146e-02
   1.4957e-01   1.4957e-01
   5.3154e-02   5.3154e-02
   4.6560e-02   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 

Relative Difference: 0.974553

Program paused. Press enter to continue.

Checking Backpropagation (w/ Regularization) ... 
   9.9072e-01  -9.2783e-03
   1.0089e+00   8.8991e-03
   9.9164e-01  -8.3601e-03
   1.0076e+00   7.6281e-03
   9.9325e-01  -6.7480e-03
   1.0000e+00  -3.0498e-06
   1.0000e+00   1.4287e-05
   9.9997e-01  -2.5938e-05
   1.0000e+00   3.6988e-05
   9.9995e-01  -4.6876e-05
   9.9982e-01  -1.7506e-04
   1.0002e+00   2.3315e-04
   9.9971e-01  -2.8747e-04
   1.0003e+00   3.3532e-04
   9.9962e-01  -3.7622e-04
   9.9990e-01  -9.6266e-05
   1.0001e+00   1.1798e-04
   9.9986e-01  -1.3715e-04
   1.0002e+00   1.5325e-04
   9.9983e-01  -1.6656e-04
   6.1454e-01   3.1454e-01
   4.1106e-01   1.1106e-01
   3.9740e-01   9.7401e-02
   4.6409e-01   1.6409e-01
   3.5757e-01   5.7574e-02
   3.5046e-01   5.0458e-02
   4.6457e-01   1.6457e-01
   3.5779e-01   5.7787e-02
   3.5075e-01   5.0753e-02
   4.5834e-01   1.5834e-01
   3.5592e-01   5.5924e-02
   3.4916e-01   4.9162e-02
   4.5113e-01   1.5113e-01
   3.5370e-01   5.3697e-02
   3.4715e-01   4.7146e-02
   4.4957e-01   1.4957e-01
   3.5315e-01   5.3154e-02
   3.4656e-01   4.6560e-02
The above two columns you get should be very similar.
(Left-Your Numerical Gradient, Right-Analytical Gradient)

If your backpropagation implementation is correct, then 
the relative difference will be small (less than 1e-9). 


Training Neural Network... 

Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 10.000000
>> ex4

Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Feedforward Using Neural Network ...
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.287629)

Program paused. Press enter to continue.

Checking Cost Function (w/ Regularization) ... 
Cost at parameters (loaded from ex4weights): 0.287629 
(this value should be about 0.383770)
Program paused. Press enter to continue.

Evaluating sigmoid gradient...
Sigmoid gradient evaluated at [1 -0.5 0 0.5 1]:
  0.196612 0.235004 0.250000 0.235004 0.196612 

Program paused. Press enter to continue.

Initializing Neural Network Parameters ...

Checking Backpropagation... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -3.0498e-06
   1.4287e-05   1.4287e-05
  -2.5938e-05  -2.5938e-05
   3.6988e-05   3.6988e-05
  -4.6876e-05  -4.6876e-05
  -1.7506e-04  -1.7506e-04
   2.3315e-04   2.3315e-04
  -2.8747e-04  -2.8747e-04
   3.3532e-04   3.3532e-04
  -3.7622e-04  -3.7622e-04
  -9.6266e-05  -9.6266e-05
   1.1798e-04   1.1798e-04
  -1.3715e-04  -1.3715e-04
   1.5325e-04   1.5325e-04

Checking Backpropagation (w/ Regularization) ... 
  -9.2783e-03  -9.2783e-03
   8.8991e-03   8.8991e-03
  -8.3601e-03  -8.3601e-03
   7.6281e-03   7.6281e-03
  -6.7480e-03  -6.7480e-03
  -3.0498e-06  -1.6768e-02
   1.4287e-05   3.9433e-02
  -2.5938e-05   5.9336e-02
   3.6988e-05   2.4764e-02
  -4.6876e-05  -3.2688e-02
  -1.7506e-04  -6.0174e-02
   2.3315e-04  -3.1961e-02
  -2.8747e-04   2.4923e-02
   3.3532e-04   5.9772e-02
  -3.7622e-04   3.8641e-02
  -9.6266e-05  -1.7370e-02
   1.1798e-04  -5.7566e-02
  -1.3715e-04  -4.5196e-02
   1.5325e-04   9.1459e-03
  -1.6656e-04   5.4610e-02
   3.1454e-01   3.1454e-01

Training Neural Network... 
Iteration    50 | Cost: 6.472341e-01
Program paused. Press enter to continue.

Visualizing Neural Network... 

Program paused. Press enter to continue.

Training Set Accuracy: 91.300000
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> il = 2;              % input layer
>> hl = 2;              % hidden layer
>> nl = 4;              % number of labels
>> nn = [ 1:18 ] / 10;  % nn_params
>> X = cos([1 2 ; 3 4 ; 5 6]);
>> y = [4; 2; 3];
>> lambda = 4;
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.89947
   1.24656
   0.37246
   0.49749
   0.64174
   0.74614
   1.81675
   1.63543
   1.78467
   1.93147
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I,J,...) = X: dimensions mismatch
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 100, column 19
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
parse error near line 100 of file /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m

  syntax error

>>> 	Theta1_grad(1,) = Delta1/m;
                   ^

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
error: nnCostFunction: A(I) = X: X must have the same size as I
error: called from:
error:   /Users/Usamahk/Documents/Courses and Books/Coursera/Machine Learning/ml-ex4/ex4/nnCostFunction.m at line 100, column 17
>> whos
Variables in the current scope:

   Attr Name                   Size                     Bytes  Class
   ==== ====                   ====                     =====  ===== 
        J                      1x1                          8  double
        Theta1                25x401                    80200  double
        Theta2                10x26                      2080  double
        X                      3x2                         48  double
        ans                    1x1                          8  double
        cost                  50x1                        400  double
        costFunction           1x1                          0  function_handle
        debug_J                1x1                          8  double
        g                      1x5                         40  double
        grad                  18x1                        144  double
        hidden_layer_size      1x1                          8  double
        hl                     1x1                          8  double
        il                     1x1                          8  double
        initial_Theta1        25x401                    80200  double
        initial_Theta2        10x26                      2080  double
        initial_nn_params  10285x1                      82280  double
        input_layer_size       1x1                          8  double
        lambda                 1x1                          8  double
        m                      1x1                          8  double
        nl                     1x1                          8  double
        nn                     1x18                       144  double
        nn_params          10285x1                      82280  double
        num_labels             1x1                          8  double
        options                1x1                          8  struct
        pred                5000x1                      40000  double
        sel                    1x100                      800  double
        y                      3x1                         24  double

Total is 46353 elements using 370816 bytes

>> Theta2(:,1)
ans =

   5.58602
  -2.04171
  -1.81419
  -0.29435
  -0.66678
  -1.11614
  -0.12592
  -2.32328
  -0.54516
  -3.58176

>> Theta2(:,2)
ans =

  -0.083231
  -0.601612
  -0.399112
  -0.139185
  -0.575806
  -0.391729
   0.185998
  -0.745479
   0.061273
  -0.875197

>> Theta2
Theta2 =

 Columns 1 through 6:

   5.5860247  -0.0832309  -1.5141890  -0.4698122   0.8247489  -1.4613890
  -2.0417064  -0.6016122   3.6186823  -0.4503361  -0.4978718  -0.0290650
  -1.8141930  -0.3991121  -3.6049697  -0.1578920  -0.3970627  -0.4123586
  -0.2943536  -0.1391854   1.2576399  -0.3883598  -0.2966237  -0.1312332
  -0.6667775  -0.5758060  -4.2628112  -0.0991320  -0.5266716  -0.3591110
  -1.1161398  -0.3917291   0.4331283  -0.0714112  -0.3571522   0.3166158
  -0.1259244   0.1859982   2.0161787  -0.6355650  -0.6040527  -0.2015081
  -2.3232780  -0.7454787  -1.1408317  -0.2204808  -0.4361579  -0.3585430
  -0.5451560   0.0612730  -2.4043738   0.1647450  -0.3786727  -0.1601380
  -3.5817571  -0.8751967   2.5423067  -0.8896421  -1.0016540  -0.0988208

 Columns 7 through 12:

  -1.6949785   0.3975231  -2.8889650   0.1889197  -4.3500552   0.3461040
   0.1361332  -0.5929317  -3.9411537  -0.2588064  -2.9959076  -0.2455480
   2.4065398  -0.2928778  -1.8271763  -0.3791541  -2.1416799  -0.5604930
   1.7890239  -0.0947673   1.0469678  -0.4412475   1.1582009  -0.8063098
  -1.7062618  -0.3462741   2.7751802  -0.6352021   1.3831760  -0.2225434
  -2.6886166  -0.4877974  -0.2472510   0.1476444   1.1871973   0.0029664
>> 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.507
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  25.346
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  39.773
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.507
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.207
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  16.940
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  19.760
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |   0 /  15 | 
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  75 / 100 | 
== 
>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.76614
   0.97990
   0.37246
   0.49749
   0.64174
   0.74614
   0.88342
   0.56876
   0.58467
   0.59814
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> [J grad] = nnCostFunction(nn, il, hl, nl, X, y, lambda)
J =  7.4070
grad =

   0.89947
   1.24656
   0.37246
   0.49749
   0.64174
   0.74614
   1.81675
   1.63543
   1.78467
   1.93147
   1.92598
   1.94462
   1.98965
   2.17855
   2.47834
   2.50225
   2.52644
   2.72233

>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |  15 /  15 | Nice work!
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |   0 /  40 | 
==                        Regularized Gradient |   0 /  10 | 
==                                   --------------------------------
==                                             |  50 / 100 | 
== 
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
!! Submission failed: unexpected error: operator *: nonconformant arguments (op1 is 3x1, op2 is 4x3)
!! Please try again later.
>> submit()
== Submitting solutions | Neural Networks Learning...
Use token from last successful submission (usamah.khan@gmail.com)? (Y/n): y
== 
==                                   Part Name |     Score | Feedback
==                                   --------- |     ----- | --------
==               Feedforward and Cost Function |  30 /  30 | Nice work!
==                   Regularized Cost Function |  15 /  15 | Nice work!
==                            Sigmoid Gradient |   5 /   5 | Nice work!
==   Neural Network Gradient (Backpropagation) |  40 /  40 | Nice work!
==                        Regularized Gradient |  10 /  10 | Nice work!
==                                   --------------------------------
==                                             | 100 / 100 | 
== 
>> 
